{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==1.13.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MpXhx68fB2Xx",
        "outputId": "372f4d81-057c-460b-c394-379488faa216"
      },
      "id": "MpXhx68fB2Xx",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==1.13.2 (from versions: 2.8.0rc0, 2.8.0rc1, 2.8.0, 2.8.1, 2.8.2, 2.8.3, 2.8.4, 2.9.0rc0, 2.9.0rc1, 2.9.0rc2, 2.9.0, 2.9.1, 2.9.2, 2.9.3, 2.10.0rc0, 2.10.0rc1, 2.10.0rc2, 2.10.0rc3, 2.10.0, 2.10.1, 2.11.0rc0, 2.11.0rc1, 2.11.0rc2, 2.11.0, 2.11.1, 2.12.0rc0, 2.12.0rc1, 2.12.0, 2.12.1, 2.13.0rc0, 2.13.0rc1, 2.13.0rc2, 2.13.0, 2.13.1, 2.14.0rc0, 2.14.0rc1, 2.14.0, 2.14.1, 2.15.0rc0, 2.15.0rc1, 2.15.0, 2.15.0.post1, 2.15.1, 2.16.0rc0, 2.16.1, 2.16.2, 2.17.0rc0, 2.17.0rc1, 2.17.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow==1.13.2\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a new environment\n",
        "#pip install tensorflow==1.13.2\n",
        "#pip install protobuf==3.20.3, if not installed properly try pip install protobuf==3.20.3 --user\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "MZR1ZGq0B8Do"
      },
      "id": "MZR1ZGq0B8Do",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.examples.tutorials.mnist import input_data #imports a specific module from TensorFlow that is designed to handle the MNIST dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "id": "BQvgVDElCzhA",
        "outputId": "caa5e026-cc23-423e-ac33-b084580900e2"
      },
      "id": "BQvgVDElCzhA",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tensorflow.examples'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-1d8f7755411f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtutorials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmnist\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minput_data\u001b[0m \u001b[0;31m#imports a specific module from TensorFlow that is designed to handle the MNIST dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.examples'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca01920d",
      "metadata": {
        "id": "ca01920d"
      },
      "source": [
        "Load MNIST Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f446c561",
      "metadata": {
        "id": "f446c561",
        "outputId": "a9e84e43-0ca3-4866-fd9a-9e42d50152dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From C:\\Users\\gshvr\\AppData\\Local\\Temp\\ipykernel_5928\\3018116216.py:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From C:\\Users\\gshvr\\anaconda3\\envs\\EXP_2_IVA\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From C:\\Users\\gshvr\\anaconda3\\envs\\EXP_2_IVA\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n",
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "WARNING:tensorflow:From C:\\Users\\gshvr\\anaconda3\\envs\\EXP_2_IVA\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "WARNING:tensorflow:From C:\\Users\\gshvr\\anaconda3\\envs\\EXP_2_IVA\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From C:\\Users\\gshvr\\anaconda3\\envs\\EXP_2_IVA\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From C:\\Users\\gshvr\\anaconda3\\envs\\EXP_2_IVA\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ]
        }
      ],
      "source": [
        "mnist_data = input_data.read_data_sets('MNIST_data', one_hot=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf051641",
      "metadata": {
        "id": "cf051641"
      },
      "outputs": [],
      "source": [
        "input_size = 784\n",
        "no_classes = 10\n",
        "batch_size = 100 # the number of images the model will process at once during each training step\n",
        "total_batches = 200 #This defines the number of batches to process during the entire training phase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9293d0a",
      "metadata": {
        "id": "e9293d0a"
      },
      "outputs": [],
      "source": [
        "#Creating Placeholders for Input Data\n",
        "\n",
        "x_input = tf.placeholder(tf.float32, shape=[None, input_size]) #put the images when we're ready to train the model\n",
        "y_input = tf.placeholder(tf.float32, shape=[None, no_classes]) #put the labels for the images"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c42ed6f4",
      "metadata": {
        "id": "c42ed6f4"
      },
      "source": [
        "Random Initialization: Starting with random values for weights and biases helps the neural network learn more effectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f4fa380",
      "metadata": {
        "id": "6f4fa380",
        "outputId": "3ae4faf1-38e6-4c7d-edf7-086e36b7fd53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From C:\\Users\\gshvr\\anaconda3\\envs\\EXP_2_IVA\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ]
        }
      ],
      "source": [
        "#This function creates a TensorFlow variable, which is a value that can change during training\n",
        "weights = tf.Variable(tf.random_normal([input_size, no_classes])) #creating a matrix of random numbers with 784 rows and 10 columns\n",
        "bias = tf.Variable(tf.random_normal([no_classes])) #This initializes the biases with random values from a normal distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12b8a3a0",
      "metadata": {
        "id": "12b8a3a0"
      },
      "outputs": [],
      "source": [
        "# Computes the raw output values (logits) of a neural network layer before applying any activation function.\n",
        "logits = tf.matmul(x_input, weights) + bias #raw output values (logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87a890f8",
      "metadata": {
        "id": "87a890f8",
        "outputId": "b094cc9f-6813-4cf8-d3ff-62ac95b297ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From C:\\Users\\gshvr\\AppData\\Local\\Temp\\ipykernel_5928\\1188355049.py:1: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#learning_rate = 0.5, optimizer will make relatively large adjustments to the weights during each training step\n",
        "softmax_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=y_input, logits=logits) # y_input: actual values\n",
        "loss_operation = tf.reduce_mean(softmax_cross_entropy) #tf.reduce_mean: a function that calculates the average of elements across a tensor\n",
        "optimiser = tf.train.GradientDescentOptimizer(learning_rate=0.5).minimize(loss_operation) #creates an optimizer that uses gradient descent with a specified learning rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b892e89d",
      "metadata": {
        "id": "b892e89d"
      },
      "outputs": [],
      "source": [
        "session = tf.Session() #opening a new workspace to run and manage computations\n",
        "session.run(tf.global_variables_initializer()) #nitializes all the variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87d4819f",
      "metadata": {
        "id": "87d4819f",
        "outputId": "6e421910-19bd-4d39-e91a-788e13656c80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "11.168253\n",
            "11.541677\n",
            "8.581201\n",
            "7.726984\n",
            "6.7552714\n",
            "6.186933\n",
            "6.4940157\n",
            "5.5688653\n",
            "5.234816\n",
            "6.325038\n",
            "4.6998076\n",
            "4.854911\n",
            "5.6954823\n",
            "5.393056\n",
            "4.2530956\n",
            "4.139567\n",
            "5.4179254\n",
            "3.8200574\n",
            "3.213886\n",
            "3.564516\n",
            "3.701602\n",
            "3.064346\n",
            "2.986631\n",
            "3.6415505\n",
            "4.0158205\n",
            "2.863676\n",
            "2.6547315\n",
            "3.8592353\n",
            "2.4801261\n",
            "3.4353814\n",
            "3.50653\n",
            "2.3396099\n",
            "2.598445\n",
            "3.0120802\n",
            "2.4712617\n",
            "1.9807243\n",
            "2.1537719\n",
            "2.6532176\n",
            "2.8181689\n",
            "3.304314\n",
            "2.3708684\n",
            "2.5694647\n",
            "2.4435596\n",
            "2.3868709\n",
            "2.307035\n",
            "2.093668\n",
            "2.3678577\n",
            "1.8891993\n",
            "1.3506464\n",
            "1.8817009\n",
            "2.7607567\n",
            "1.9010379\n",
            "2.0797036\n",
            "2.3432305\n",
            "1.552629\n",
            "1.9668562\n",
            "1.9670178\n",
            "1.7511559\n",
            "2.323933\n",
            "1.6322157\n",
            "1.3550305\n",
            "2.1261764\n",
            "1.4712453\n",
            "2.5142212\n",
            "1.9959999\n",
            "1.9623836\n",
            "2.191228\n",
            "1.7140014\n",
            "1.1175255\n",
            "1.2920209\n",
            "2.0851235\n",
            "1.6982138\n",
            "1.5527416\n",
            "1.7128235\n",
            "2.1003377\n",
            "1.7438765\n",
            "2.0092623\n",
            "1.6178325\n",
            "1.9688448\n",
            "2.3812962\n",
            "1.4439485\n",
            "1.4644741\n",
            "1.474359\n",
            "1.1277419\n",
            "1.6927321\n",
            "1.9912019\n",
            "1.3968406\n",
            "1.4554831\n",
            "1.5328145\n",
            "1.7240487\n",
            "1.9166992\n",
            "1.4951501\n",
            "2.006142\n",
            "1.9362488\n",
            "1.4261911\n",
            "1.8034385\n",
            "1.2269171\n",
            "1.3571419\n",
            "2.0006058\n",
            "1.3887614\n",
            "1.0937004\n",
            "2.0488517\n",
            "1.3586357\n",
            "1.4034848\n",
            "1.9782399\n",
            "1.8760391\n",
            "1.0694042\n",
            "1.6552911\n",
            "1.4748386\n",
            "0.9207703\n",
            "1.3306935\n",
            "1.415452\n",
            "1.1968713\n",
            "1.4652128\n",
            "1.3029882\n",
            "1.1829888\n",
            "1.1751709\n",
            "0.8645092\n",
            "1.3401489\n",
            "1.5019679\n",
            "1.2903591\n",
            "1.0433326\n",
            "1.2803633\n",
            "1.4596231\n",
            "0.8759705\n",
            "1.527789\n",
            "0.91860956\n",
            "0.84747344\n",
            "1.523406\n",
            "2.1560614\n",
            "1.2940314\n",
            "1.0525346\n",
            "1.0438226\n",
            "0.96243113\n",
            "1.0451698\n",
            "1.3729779\n",
            "1.1907315\n",
            "1.3127687\n",
            "1.1655718\n",
            "1.4249606\n",
            "0.8689621\n",
            "1.0648916\n",
            "1.4143597\n",
            "1.366\n",
            "1.2661849\n",
            "1.3204396\n",
            "1.2428728\n",
            "0.8096941\n",
            "1.3287761\n",
            "1.548773\n",
            "1.2260001\n",
            "1.618959\n",
            "0.9316904\n",
            "1.217738\n",
            "1.4518867\n",
            "0.8558938\n",
            "1.3019996\n",
            "1.5303406\n",
            "1.0231948\n",
            "0.6018983\n",
            "1.0646584\n",
            "1.402006\n",
            "1.0995135\n",
            "0.8324411\n",
            "0.931968\n",
            "1.151199\n",
            "1.3680551\n",
            "1.6013283\n",
            "1.2265918\n",
            "0.9771304\n",
            "1.1440374\n",
            "1.6682163\n",
            "1.362256\n",
            "0.8321362\n",
            "1.2431072\n",
            "1.0689564\n",
            "1.0244613\n",
            "0.76085466\n",
            "0.86916435\n",
            "1.3591069\n",
            "0.42628312\n",
            "0.89296526\n",
            "1.0416824\n",
            "1.2355263\n",
            "1.1209749\n",
            "1.1192315\n",
            "1.010484\n",
            "1.0292883\n",
            "1.3625866\n",
            "0.9340856\n",
            "1.0199308\n",
            "0.95198935\n",
            "1.3050162\n",
            "1.0151584\n",
            "1.6905084\n",
            "0.69172525\n",
            "0.7277572\n",
            "0.7200901\n",
            "0.6008954\n",
            "1.4706861\n"
          ]
        }
      ],
      "source": [
        "#The feed_dict argument is used to pass data to the placeholders in your computation graph during a session run\n",
        "for batch_no in range(total_batches):\n",
        "    mnist_batch = mnist_data.train.next_batch(batch_size)\n",
        "    train_images, train_labels = mnist_batch[0], mnist_batch[1]\n",
        "    _, loss_value = session.run([optimiser, loss_operation], feed_dict={\n",
        "        x_input: train_images,\n",
        "        y_input: train_labels\n",
        "    })\n",
        "    print(loss_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4bbb53e",
      "metadata": {
        "id": "f4bbb53e",
        "outputId": "7f814b4a-8215-453e-f460-f44f2b064464"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy :  0.8039\n"
          ]
        }
      ],
      "source": [
        "predictions = tf.argmax(logits, 1) #Axis 1 refers to the classes dimension.\n",
        "correct_predictions = tf.equal(predictions, tf.argmax(y_input, 1))\n",
        "accuracy_operation = tf.reduce_mean(tf.cast(correct_predictions,tf.float32))\n",
        "test_images, test_labels = mnist_data.test.images, mnist_data.test.labels\n",
        "accuracy_value = session.run(accuracy_operation, feed_dict={\n",
        "    x_input: test_images,\n",
        "    y_input: test_labels\n",
        "})\n",
        "print('Accuracy : ', accuracy_value)\n",
        "session.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c3e7a4f",
      "metadata": {
        "id": "7c3e7a4f"
      },
      "source": [
        "# Try your own code : sample code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd053e73",
      "metadata": {
        "id": "dd053e73",
        "outputId": "6821cb07-e20f-4889-e0e4-e4b2aac3d67c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "Accuracy :  0.922\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "# Load the MNIST dataset\n",
        "mnist_data = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
        "\n",
        "# Define placeholders for inputs and labels\n",
        "x_input = tf.placeholder(tf.float32, shape=[None, 784])\n",
        "y_input = tf.placeholder(tf.float32, shape=[None, 10])\n",
        "\n",
        "# Define a simple neural network\n",
        "W = tf.Variable(tf.zeros([784, 10]))\n",
        "b = tf.Variable(tf.zeros([10]))\n",
        "logits = tf.matmul(x_input, W) + b\n",
        "predictions = tf.nn.softmax(logits)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_input, logits=logits))\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.5).minimize(loss)\n",
        "\n",
        "# Define accuracy calculation\n",
        "correct_predictions = tf.equal(tf.argmax(predictions, 1), tf.argmax(y_input, 1))\n",
        "accuracy_operation = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))\n",
        "\n",
        "# Initialize variables\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Start a TensorFlow session\n",
        "with tf.Session() as session:\n",
        "    session.run(init)\n",
        "\n",
        "    # Training\n",
        "    for epoch in range(10):\n",
        "        for _ in range(mnist_data.train.num_examples // 100):\n",
        "            batch_x, batch_y = mnist_data.train.next_batch(100)\n",
        "            session.run(optimizer, feed_dict={x_input: batch_x, y_input: batch_y})\n",
        "\n",
        "    # Testing\n",
        "    test_images, test_labels = mnist_data.test.images, mnist_data.test.labels\n",
        "    accuracy_value = session.run(accuracy_operation, feed_dict={x_input: test_images, y_input: test_labels})\n",
        "    print('Accuracy : ', accuracy_value)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd841604",
      "metadata": {
        "id": "dd841604"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.16"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}